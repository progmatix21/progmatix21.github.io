<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Understand the Confusion Matrix - Progmatix 21</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="Progmatix 21" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">Progmatix 21</div>
					<div class="logo__tagline">My learnings: byte by byte</div>
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Understand the Confusion Matrix</h1>
			<p class="post__lead">Blow away the confusion</p>
			<div class="post__meta meta">
<div class="meta__item-datetime meta__item">
	<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<svg version="1.1" id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" class="meta__icon icon icon-calendar" width="18" height="16" 
	 viewBox="0 0 458 458" style="enable-background:new 0 0 458 458;" xml:space="preserve">
<g>
	<path d="M111.938,135.598c11.046,0,20-8.954,20-20V21.597c0-11.046-8.954-20-20-20s-20,8.954-20,20v94.001
		C91.938,126.644,100.892,135.598,111.938,135.598z"/>
	<path d="M346.063,135.598c11.046,0,20-8.954,20-20V21.597c0-11.046-8.954-20-20-20s-20,8.954-20,20v94.001
		C326.063,126.644,335.017,135.598,346.063,135.598z"/>
	<path d="M443,82.403h-46.938c0,12.42,0,20.918,0,33.195c0,27.614-22.386,50-50,50c-27.614,0-50-22.386-50-50
		c0-12.281,0-20.771,0-33.195H161.938c0,12.42,0,20.918,0,33.195c0,27.614-22.386,50-50,50c-27.614,0-50-22.386-50-50
		c0-12.281,0-20.771,0-33.195H15c-8.284,0-15,6.716-15,15v89.641h458V97.403C458,89.119,451.284,82.403,443,82.403z"/>
	<path d="M0,441.403c0,8.284,6.716,15,15,15h428c8.284,0,15-6.716,15-15V227.044H0V441.403z M244.191,313.046
		c0-6.036,3.59-11.504,9.127-13.907c7.49-3.249,16.316-9.028,21.176-13.393c2.782-2.499,6.38-3.88,10.121-3.88h19.254
		c4.395,0,7.957,3.563,7.957,7.958v80.903h11.445c8.275,0,14.984,6.708,14.984,14.984c0,8.275-6.708,14.984-14.984,14.984h-59.934
		c-8.275,0-14.984-6.708-14.984-14.984c0-8.275,6.708-14.984,14.984-14.984h14.567V317.66c-2.163,2.886-6.797,6.077-12.243,8.754
		c-4.618,2.27-10.076,2.008-14.445-0.711c-4.369-2.718-7.025-7.501-7.025-12.647V313.046z M123.295,390.677
		c2.691-29.175,18.127-37.168,47.125-52.83c4.622-2.493,12.811-7.06,15.92-11.134c3.842-5.044,1.97-18.001-14.359-18.001
		c-5.549,0-11.094,1.106-17.704,5.967c-6.262,4.605-14.984,3.637-20.067-2.244l-0.594-0.688c-3.026-3.501-4.246-8.216-3.31-12.747
		s3.925-8.371,8.09-10.387c22.329-10.811,56.654-13.101,73.853,0.952c7.977,6.523,11.966,15.332,11.966,26.43
		c0,32.434-40.129,38.997-54.315,54.731h41.622c8.275,0,14.984,6.708,14.984,14.984c0,8.275-6.708,14.984-14.984,14.984h-79.1
		c-2.577,0-5.036-1.095-6.772-3C123.914,395.789,123.059,393.243,123.295,390.677z"/>
</g>
</svg>
<time class="meta__text" datetime="2023-07-14T11:51:17&#43;05:30">July 14, 2023</time></div>
<div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2l1 2h8v11h-16v-13z"/></svg><span class="meta__text"><a class="meta__link" href="/categories/data-science/" rel="category">Data Science</a>
	</span>
</div><div class="meta__item-categories meta__item">
	<span class="meta__text">
		<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>1101 words/6 min read
	</span>
</div>
</div>
		</header>
		
		
		
	<figure class="post__thumbnail thumbnail">
		
		<img class="thumbnail__image" src="/img/confusion_matrix.png" alt="Understand the Confusion Matrix">
		
	</figure>
<div class="post__toc toc">
	<div class="toc__title">Page content</div>
	<div class="toc__menu">
	    <details>
	        <summary>Click to toggle</summary>
		        <nav id="TableOfContents">
  <ul>
    <li><a href="#listen----this-post-explained">Listen &ndash; this post explained</a></li>
    <li><a href="#what-is-a-confusion-matrix">What is a confusion matrix?</a></li>
    <li><a href="#building-the-confusion-matrix">Building the confusion matrix</a>
      <ul>
        <li><a href="#starting-point">Starting point</a></li>
        <li><a href="#bringing-in-the-classifier">Bringing in the classifier</a></li>
      </ul>
    </li>
    <li><a href="#what-do-these-numbers-mean">What do these numbers mean?</a></li>
    <li><a href="#insights-from-the-confusion-matrix">Insights from the confusion matrix</a></li>
    <li><a href="#interpreting-the-metrics">Interpreting the metrics</a></li>
    <li><a href="#confusion-matrix-in-the-wild">Confusion matrix in the wild</a></li>
  </ul>
</nav>
		    </details>
	</div>
</div>
<div class="content post__content clearfix">
			<style>
#myBtn {
  display: none;
  position: fixed;
  bottom: 20px;
  right: 30px;
  z-index: 99;
  font-size: 36px;
  font-weight: bold;
  border: none;
  outline: none;
  background-color: #444;
  color: white;
  cursor: pointer;
  padding: 15px;
  border-radius: 50%;
}

#myBtn:hover {
  background-color: #008080;
}
</style>

<button onclick="topFunction()" id="myBtn" title="Go to top">&#9650;</button>


<script>

let mybutton = document.getElementById("myBtn");


window.onscroll = function() {scrollFunction()};

function scrollFunction() {
  if (document.body.scrollTop > 1000 || document.documentElement.scrollTop > 1000) {
    mybutton.style.display = "block";
  } else {
    mybutton.style.display = "none";
  }
}


function topFunction() {
  document.body.scrollTop = 0;
  document.documentElement.scrollTop = 0;
}
</script>



<p>You&rsquo;ve created a classification model and come across a new concept called
confusion matrix.  However tough it may seem, a classification model evaluation
is not complete unless you add in your confusion matrix.</p>
<h2 id="listen----this-post-explained">Listen &ndash; this post explained</h2>
<iframe src="https://archive.org/embed/confusion_matrix" width="500" height="60" frameborder="0" webkitallowfullscreen="true" mozallowfullscreen="true" allowfullscreen></iframe>
<h2 id="what-is-a-confusion-matrix">What is a confusion matrix?</h2>
<p>A confusion matrix is commonly, a 2x2 matrix for a binary classification problem
containing numbers pertaining to the classification.  It becomes an nxn matrix for
an n class classification problem.  What are these numbers and how do they come
about?  Let us take the example of a binary classification problem.</p>
<p>Let us take a contrived example of a parts bin which has both defective and
good parts.  We have a classifier that will detect defective parts based
on certain parameters that will be part of a dataset.  We need to evaluate how
good the classifier is.  Note that these parts are labelled good or defective
so that we can evaluate the classifier prediction against the true label.</p>
<p>We will assume a total of 100 parts of which 60 are good and 40 are defective.
Since our goal is to identify defective parts, we label them as 1.</p>
<h2 id="building-the-confusion-matrix">Building the confusion matrix</h2>
<p>Let us build the confusion matrix in two steps</p>
<h3 id="starting-point">Starting point</h3>
<p>Before we bring in the classifier, this is how things look:</p>
<pre><code>           1      40
True label    ----------
           0      60
</code></pre><h3 id="bringing-in-the-classifier">Bringing in the classifier</h3>
<p>As we bring in the classifier, it classifies our parts as good or defective.
Ideally, we would expect our classifier to correctly identify all the good
parts and only the good parts correctly.  However, no classifier is perfect and
that is the whole point of the evaluation.  Our classifier makes mistakes too.</p>
<p>After this exercise, we have two versions of good
and defective &ndash; one which is the true label and another is the one as labelled
by the classifier.  To represent this, we can split the above table vertically,
giving us four cells, one each for a combination of the true and predicted label.</p>
<p>Let&rsquo;s assume our classifier has predicted the labels.  We can then represent this
prediction along with the true labels as follows:</p>
<pre><code>             1  30 | 10
True label     ----|----
             0  20 | 40
                1    0
             Predicted Label
</code></pre><p>This is nothing but the confusion matrix.</p>
<h2 id="what-do-these-numbers-mean">What do these numbers mean?</h2>
<p>To understand what these numbers mean, let us superimpose the following legend
on our confusion matrix above.</p>
<pre><code>             1  TP | FN
True label     ----|----
             0  FP | TN
                1    0
             Predicted Label
</code></pre><p>In the above confusion matrix, the criss-crossing of the true label (1,0) along
with the predicted label (1,0) gives rise to four values: True Positives for (1,1),
True Negatives for (0,0), False Positives for (0,1) and False Negatives for (1,0).</p>
<p>From our confusion matrix, we can deduce that:</p>
<ul>
<li>The classifier has correctly identified 30 defective parts(i.e. true defective
and classified as defective).  These are the <strong>true positives</strong>.</li>
<li>However, it has not been able to identify 10 parts as defective.  Rather, they
have been falsely classified as good parts.  These are the <strong>false negatives</strong>.</li>
<li>Looking at the cell corresponding to the labels (0,0), we observe that we have
correctly classified 40 parts as belonging to class 0 (good parts).  These are
our <strong>true negatives</strong>.</li>
<li>Again, the figure 20 (cell (0,1)) tells us that 20 good parts have been
falsely identified as bad.  These are our <strong>false positives</strong>.</li>
</ul>
<h2 id="insights-from-the-confusion-matrix">Insights from the confusion matrix</h2>
<p>Now, that we have drawn our confusion matrix, let us see how can we put the four values to
good use.  They can be used to calculate the following metrics:</p>
<ol>
<li><strong>Accuracy:</strong> The ratio of instances correctly classified of the total instances.
Formula: <code>(TP+TN)/(TP+TN+FP+FN)</code></li>
<li><strong>Precision:</strong> The ratio of true positive instances out of all the instances classified
as positive by the classifier. Formula: <code>TP/(TP+FP)</code></li>
<li><strong>Recall:</strong> (a.k.a sensitivity)The true (identified) positives as a ratio of
the total positives in the dataset.
Recall is also known as True Positive Rate or TPR.  Formula: <code>TP/(TP+FN)</code></li>
<li><strong>FPR (False Positive Rate):</strong> The False Positive Rate is the ratio of False Positives by
the total negatives. Formula: <code>FP/(FP+TN)</code> or <code>1-Specificity</code></li>
<li><strong>Specificity:</strong> Specificity is the capability of the classifier to point out the
negatives from the data.  It is the equivalent of recall for the negative class.
Formula: <code>TN/(TN+FP)</code></li>
</ol>
<p><strong>Additional learnings:</strong></p>
<p>Two additional insights can be easily derived from the confusion matrix:</p>
<ul>
<li><strong>Is there a class imbalance?</strong> Comparing <code>TP+FN</code> to <code>FP+TN</code> tells us if there
is a class imbalance.</li>
<li><strong>Odds ratio:</strong> Odds ratio tells us how much better our classifier is compared
to a dummy classifier making random guesses.  A low odds ratio might lead you to
question the need for the classifier at all.  Odds ratio is given by:
<code>Recall/(1-Specificity)</code>.</li>
</ul>
<h2 id="interpreting-the-metrics">Interpreting the metrics</h2>
<p>Ordinarily, the ability for a classifier to perform well can be denoted by means
of accuracy.  For highly imbalanced datasets, accuracy is not a good measure of
the classification.  When accuracy is applied to an imbalanced dataset, the majority
class predominates and gives high but misleading accuracy values.  Precision and
recall are more representative of the classifier performance.</p>
<h2 id="confusion-matrix-in-the-wild">Confusion matrix in the wild</h2>
<ul>
<li>If you look at a search engine as a classifier, it has relatively low precision
and an unknown recall (why?).  What saves it is the fact that it ranks results
for us.</li>
<li>In rare disease identification, recall or sensitivity is the primary measure of
importance.</li>
<li>In fraud detection, banks might insist on high precision for fear of false
positives that might lead to honest customers being troubled.</li>
<li>The legal system in a country might operate under the principle of &lsquo;no innocent
shall be punished&rsquo;.  This translates to high precision, but poor recall (indicated
by the seemingly guilty getting away).</li>
<li>A police blockade set up for identifying fugitives will work for near 100%
recall and shall not be bothered about precision; in fact they might insist on
checking each and every vehicle.</li>
</ul>
<p>To see a confusion matrix in context, refer
to this Kaggle <a href="https://www.kaggle.com/code/atrijtalgery/redwine-classif-n-with-precision-recall-tradeoff">notebook</a>.</p>
<blockquote>
<p><strong>Finally, some intuition to remember the confusion matrix:</strong>
The task of a classifier is to get as many positive instances as it can to the true positive
side, while keeping as many negative instances from entering the false positive side.</p>
</blockquote>
		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/classification/" rel="tag">classification</a>
		</li>
	</ul>
</div>
		</footer>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/conda-takes-forever/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Conda Takes Forever</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/learning-curve/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Learning Curve</p>
		</a>
	</div>
</nav>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
<div class="footer__links">
	<a class="footer__link" href="/about/">About</a>
</div>
		<div class="footer__copyright">
			&copy; 2025 Atrij Talgery.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
</body>
</html>