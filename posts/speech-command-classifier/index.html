<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Speech Command Classifier - Progmatix 21</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="Progmatix 21" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">Progmatix 21</div>
					<div class="logo__tagline">My learnings: byte by byte</div>
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Speech Command Classifier</h1>
			<p class="post__lead">Can a classifier be as good as a human at classifying speech commands?</p>
			<div class="post__meta meta">
<div class="meta__item-datetime meta__item">
	<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<svg version="1.1" id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" class="meta__icon icon icon-calendar" width="18" height="16" 
	 viewBox="0 0 458 458" style="enable-background:new 0 0 458 458;" xml:space="preserve">
<g>
	<path d="M111.938,135.598c11.046,0,20-8.954,20-20V21.597c0-11.046-8.954-20-20-20s-20,8.954-20,20v94.001
		C91.938,126.644,100.892,135.598,111.938,135.598z"/>
	<path d="M346.063,135.598c11.046,0,20-8.954,20-20V21.597c0-11.046-8.954-20-20-20s-20,8.954-20,20v94.001
		C326.063,126.644,335.017,135.598,346.063,135.598z"/>
	<path d="M443,82.403h-46.938c0,12.42,0,20.918,0,33.195c0,27.614-22.386,50-50,50c-27.614,0-50-22.386-50-50
		c0-12.281,0-20.771,0-33.195H161.938c0,12.42,0,20.918,0,33.195c0,27.614-22.386,50-50,50c-27.614,0-50-22.386-50-50
		c0-12.281,0-20.771,0-33.195H15c-8.284,0-15,6.716-15,15v89.641h458V97.403C458,89.119,451.284,82.403,443,82.403z"/>
	<path d="M0,441.403c0,8.284,6.716,15,15,15h428c8.284,0,15-6.716,15-15V227.044H0V441.403z M244.191,313.046
		c0-6.036,3.59-11.504,9.127-13.907c7.49-3.249,16.316-9.028,21.176-13.393c2.782-2.499,6.38-3.88,10.121-3.88h19.254
		c4.395,0,7.957,3.563,7.957,7.958v80.903h11.445c8.275,0,14.984,6.708,14.984,14.984c0,8.275-6.708,14.984-14.984,14.984h-59.934
		c-8.275,0-14.984-6.708-14.984-14.984c0-8.275,6.708-14.984,14.984-14.984h14.567V317.66c-2.163,2.886-6.797,6.077-12.243,8.754
		c-4.618,2.27-10.076,2.008-14.445-0.711c-4.369-2.718-7.025-7.501-7.025-12.647V313.046z M123.295,390.677
		c2.691-29.175,18.127-37.168,47.125-52.83c4.622-2.493,12.811-7.06,15.92-11.134c3.842-5.044,1.97-18.001-14.359-18.001
		c-5.549,0-11.094,1.106-17.704,5.967c-6.262,4.605-14.984,3.637-20.067-2.244l-0.594-0.688c-3.026-3.501-4.246-8.216-3.31-12.747
		s3.925-8.371,8.09-10.387c22.329-10.811,56.654-13.101,73.853,0.952c7.977,6.523,11.966,15.332,11.966,26.43
		c0,32.434-40.129,38.997-54.315,54.731h41.622c8.275,0,14.984,6.708,14.984,14.984c0,8.275-6.708,14.984-14.984,14.984h-79.1
		c-2.577,0-5.036-1.095-6.772-3C123.914,395.789,123.059,393.243,123.295,390.677z"/>
</g>
</svg>
<time class="meta__text" datetime="2024-07-28T13:00:53&#43;05:30">July 28, 2024</time></div>
<div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2l1 2h8v11h-16v-13z"/></svg><span class="meta__text"><a class="meta__link" href="/categories/deep-learning/" rel="category">deep learning</a>
	</span>
</div><div class="meta__item-categories meta__item">
	<span class="meta__text">
		<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>1360 words/7 min read
	</span>
</div>
</div>
		</header>
		
<div class="post__toc toc">
	<div class="toc__title">Page content</div>
	<div class="toc__menu">
	    <details>
	        <summary>Click to toggle</summary>
		        <nav id="TableOfContents">
  <ul>
    <li><a href="#approach">Approach</a></li>
    <li><a href="#description-of-the-neural-network">Description of the neural network</a></li>
    <li><a href="#trainingvalidation-loss-curves">Training/Validation loss curves</a></li>
    <li><a href="#decoding-the-cnn-output">Decoding the CNN output</a></li>
    <li><a href="#confusion-matrix">Confusion matrix</a></li>
    <li><a href="#conclusion">Conclusion</a></li>
  </ul>
</nav>
		    </details>
	</div>
</div>
<div class="content post__content clearfix">
			
<style>
#myBtn {
  display: none;
  position: fixed;
  bottom: 20px;
  right: 30px;
  z-index: 99;
  font-size: 36px;
  font-weight: bold;
  border: none;
  outline: none;
  background-color: #444;
  color: white;
  cursor: pointer;
  padding: 15px;
  border-radius: 50%;
}

#myBtn:hover {
  background-color: #008080;
}
</style>

<button onclick="topFunction()" id="myBtn" title="Go to top">&#9650;</button>


<script>

let mybutton = document.getElementById("myBtn");


window.onscroll = function() {scrollFunction()};

function scrollFunction() {
  if (document.body.scrollTop > 1000 || document.documentElement.scrollTop > 1000) {
    mybutton.style.display = "block";
  } else {
    mybutton.style.display = "none";
  }
}


function topFunction() {
  document.body.scrollTop = 0;
  document.documentElement.scrollTop = 0;
}
</script>



<p>Here, we explore a workflow to build a speech command classifier based on a
Kaggle dataset.  The workflow is implemented as a Kaggle notebook.
The dataset contains 20 main words that are spoken at
least 5 times by different people.  These are commands
spoken by people for a duration of 1 second.
There are 10 additional words spoken by people.
These additional words can be optionally used to generalize
the classifier and prevent overfitting.</p>
<p>The goal of the workflow is to train the classifier in a robust enough
manner to give the same (training level) accuracy on words spoken by people not
associated with the dataset.</p>
<h2 id="approach">Approach</h2>
<p>Our dataset contains audio commands uttered by
different speakers with multiple commands uttered by the same speaker.  In
this situation, special care has to be taken for trifurcation of the dataset
to prevent data leakage.</p>
<p>A data loader is essential to manage memory and training speed requirements.  Audio augmentation
is also essential to make the classifier smart enough to recognize commands
by speakers not in the dataset or uttered in a noisy environment.</p>
<p>As mentioned, our dataset consists of 20 core commands and 10 auxiliary commands.
We remove commands labelled from &ldquo;zero&rdquo; to &ldquo;nine&rdquo; and the auxiliary commands
for the sake of keeping our time and resource requirements at a reasonable
level.</p>
<p>We now delve deeper into our implementation.</p>
<p><strong>The need for a word filter</strong>
A <code>word_filter()</code> function removes the word command files we don&rsquo;t need from
the data file list.  We exploit the structure of the dataset where we have a folder to
command correspondence.</p>
<p><strong>Trifurcation of the dataset</strong>
Our dataset has to be split into training, testing and validation sets
based on typical split ratios &ndash; we use 80-10-10.  Most importantly, we
have to maintain uniformity in our trifurcation and make sure that all words
are equally distributed in the split out dataset.  We use the property<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> of the
hash function to distribute the audio files into the three sets with some
special considerations.
We have to split our dataset based on the speaker rather than on
the words.  This ensures equal availability of words (labels) in the train, validation
and test sets ensuring that the utterances by a particular speaker of a specific command,
all fall into one set completely.  This prevents data leakage and also ensures
stable trifurcation even if the dataset is augmented later.</p>
<p>This approach has many benefits such as a stable split when new data is added
to the dataset.  There is no leakage of data from the test and validation sets
into the training set when this approach is used.  Changes in the data is not
going to disturb the splitting and is always stable.</p>
<p><strong>Feature extraction/engineering</strong>
Features are extracted as mel-spectrograms.  Thanks to them, just as we use
CNN in image classification, we can treat sound as &lsquo;pictorial&rsquo; representations
and analyze them as images.  The pictorial representations are our spectrograms.
The spectrogram is a 1 layer image with pseudo colouring.</p>
<p><img src="spectrogram.png" alt="spectrogram for word &lsquo;no&rsquo;"></p>
<p>The y-axis of the spectrogram is a (human-perceived) pitch scale denoted by &lsquo;mels&rsquo; and
the x-axis denotes the time scale.  The color map shows the energy in a particular
mel-band at a specific instance of time indexed by the color scale on the right.</p>
<p><strong>Data Augmentation</strong>
Data augmentation is absolutely essential for robust training.  We have
implemented three levels of data-augmentation keeping in mind resource requirements.
The classifier can be trained with the following three levels of augmentation:</p>
<ul>
<li><strong>no augmentation at all:</strong> this is the default way in which you train your
classifier by feeding it with unaugmented spectrograms of unaugmented audio.</li>
<li><strong>basic augmentation:</strong> use this if you want to feed unaugmented audio followed by
augmented spectrogram to your classifier.</li>
<li><strong>full augmentation:</strong> You can fully augment your data by first augmenting the
audio track, generating a plain spectrogram, augmenting the spectrogram, and
feeding it to the classifier.</li>
</ul>
<p><strong>Using a batch loader</strong>
Data loading can be a bottleneck for which it is safer to use options given
by the Keras multithreaded library.  By inheriting from
the Keras <code>Sequence</code> class and enabling multiprocessing, we can do our bit
to speed up the loading and advance batch preparation.</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#008000;font-weight:bold">class</span> <span style="color:#00f;font-weight:bold">AudioDataLoader</span>(Sequence):
    <span style="color:#008000;font-weight:bold">def</span> __init__(self, filepaths, batch_size, shuffle<span style="color:#666">=</span>True,aug<span style="color:#666">=</span>None):
        
        <span style="color:#008000">super</span>()<span style="color:#666">.</span>__init__(use_multiprocessing<span style="color:#666">=</span>True, workers<span style="color:#666">=</span><span style="color:#666">8</span>, max_queue_size<span style="color:#666">=</span><span style="color:#666">20</span>)
        
        <span style="color:#408080;font-style:italic"># aug will be passed onto audio augmenter; can be None, &#39;basic&#39; or &#39;full&#39;.</span>
        self<span style="color:#666">.</span>_filepaths <span style="color:#666">=</span> filepaths   <span style="color:#408080;font-style:italic"># All files, filepath includes label</span>
        self<span style="color:#666">.</span>_batch_size <span style="color:#666">=</span> batch_size
        self<span style="color:#666">.</span>_labels <span style="color:#666">=</span> [fname<span style="color:#666">.</span>split(<span style="color:#ba2121">&#34;/&#34;</span>)[<span style="color:#666">-</span><span style="color:#666">2</span>] <span style="color:#008000;font-weight:bold">for</span> fname <span style="color:#a2f;font-weight:bold">in</span> self<span style="color:#666">.</span>_filepaths]
        self<span style="color:#666">.</span>_shuffle <span style="color:#666">=</span> shuffle
        self<span style="color:#666">.</span>_aug <span style="color:#666">=</span> aug
        
        <span style="color:#408080;font-style:italic"># Create a label lookup to encode string labels to one-hot</span>
        self<span style="color:#666">.</span>label_lookup_ <span style="color:#666">=</span> StringLookup(
            vocabulary<span style="color:#666">=</span><span style="color:#008000">sorted</span>(<span style="color:#008000">list</span>(<span style="color:#008000">set</span>(self<span style="color:#666">.</span>_labels))),
            output_mode<span style="color:#666">=</span><span style="color:#ba2121">&#34;one_hot&#34;</span>,
            num_oov_indices<span style="color:#666">=</span><span style="color:#666">0</span>
            )
        self<span style="color:#666">.</span>on_epoch_end()  <span style="color:#408080;font-style:italic"># shuffle data before starting</span>
        
    <span style="color:#008000;font-weight:bold">def</span> <span style="color:#00f">_prep_labels</span>(self,labels):  <span style="color:#408080;font-style:italic"># Takes list of text labels, returns one-hot encoded list</span>
        <span style="color:#008000;font-weight:bold">return</span> self<span style="color:#666">.</span>label_lookup_(labels)
        
    <span style="color:#008000;font-weight:bold">def</span> __len__(self):   <span style="color:#408080;font-style:italic"># gives the number of batches</span>
        <span style="color:#008000;font-weight:bold">return</span> <span style="color:#008000">int</span>(np<span style="color:#666">.</span>ceil(<span style="color:#008000">len</span>(self<span style="color:#666">.</span>_filepaths) <span style="color:#666">/</span> <span style="color:#008000">float</span>(self<span style="color:#666">.</span>_batch_size)))

    <span style="color:#008000;font-weight:bold">def</span> <span style="color:#00f">on_epoch_end</span>(self):
        <span style="color:#ba2121">&#39;shuffle data&#39;</span>
        <span style="color:#008000;font-weight:bold">if</span> self<span style="color:#666">.</span>_shuffle:
            self<span style="color:#666">.</span>_labels, self<span style="color:#666">.</span>_filepaths <span style="color:#666">=</span> shuffle(self<span style="color:#666">.</span>_labels, self<span style="color:#666">.</span>_filepaths, random_state<span style="color:#666">=</span><span style="color:#666">34</span>)

    <span style="color:#008000;font-weight:bold">def</span> __getitem__(self, idx):
        batch_x <span style="color:#666">=</span> self<span style="color:#666">.</span>_filepaths[idx <span style="color:#666">*</span> self<span style="color:#666">.</span>_batch_size:(idx <span style="color:#666">+</span> <span style="color:#666">1</span>) <span style="color:#666">*</span> self<span style="color:#666">.</span>_batch_size]
        batch_y <span style="color:#666">=</span> self<span style="color:#666">.</span>_labels[idx <span style="color:#666">*</span> self<span style="color:#666">.</span>_batch_size:(idx <span style="color:#666">+</span> <span style="color:#666">1</span>) <span style="color:#666">*</span> self<span style="color:#666">.</span>_batch_size]
        
        prepped_batch_x <span style="color:#666">=</span> np<span style="color:#666">.</span>array([np<span style="color:#666">.</span>expand_dims(prep_audio(<span style="color:#666">*</span>librosa<span style="color:#666">.</span>load(fpath,sr<span style="color:#666">=</span>None),self<span style="color:#666">.</span>_aug),axis<span style="color:#666">=</span><span style="color:#666">2</span>) <span style="color:#008000;font-weight:bold">for</span> fpath <span style="color:#a2f;font-weight:bold">in</span> batch_x])
        prepped_batch_y <span style="color:#666">=</span> self<span style="color:#666">.</span>_prep_labels(batch_y)   <span style="color:#408080;font-style:italic">#.numpy()</span>
        <span style="color:#008000;font-weight:bold">return</span> prepped_batch_x, prepped_batch_y
</code></pre></div><h2 id="description-of-the-neural-network">Description of the neural network</h2>
<p>We make use of a CNN for our classifier.  Every sound in our dataset is
converted into &lsquo;pictorial&rsquo; representations called spectrograms which can then be
analyzed like images.
Our CNN consists of convolutional layers interlaced with LeakyReLU layers,
Global max pooling layers and Global average pooling layers.
LeakyReLU helps in convergence by preventing a dead neuron.
We favour Global average pooling as it prevents overfitting.
The input shape of the neural network is (64,64,1) which is the same as our
spectrogram size.  The image is downsampled each time it passes through the
max pooling layer and flattens the output after it passes through the Global
average pooling layer.</p>
<p>Since it is difficult to estimate the number of training epochs, we start with
a high enough number and install two callbacks to assist our training process and
stop at the right time:</p>
<ul>
<li>EarlyStopping</li>
<li>ReduceLROnPlateau</li>
</ul>
<p><img src="neural_network_layers.png" alt="Neural network layers"></p>
<h2 id="trainingvalidation-loss-curves">Training/Validation loss curves</h2>
<p>The training and validation data get loss and accuracy values which can be
plotted in two graphs.  Both of them show the extent of how loss and accuracy
improve over time and where they stop showing improvement.  The loss curve
does not show any signs of overfitting.</p>
<p><img src="loss_curve.png" alt="Loss curve"></p>
<h2 id="decoding-the-cnn-output">Decoding the CNN output</h2>
<p>The following code fragment shows how we proceed from predictions (floating
point numbers from the softmax output), to class labels.</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#408080;font-style:italic"># Get the predictions</span>
predictions <span style="color:#666">=</span> model<span style="color:#666">.</span>predict(x<span style="color:#666">=</span>test_x, batch_size<span style="color:#666">=</span><span style="color:#666">16</span>)

<span style="color:#408080;font-style:italic"># Convert softmax output to labels</span>
y_pred_labels <span style="color:#666">=</span> tf<span style="color:#666">.</span>one_hot(tf<span style="color:#666">.</span>argmax(predictions, axis <span style="color:#666">=</span> <span style="color:#666">1</span>), depth <span style="color:#666">=</span> <span style="color:#666">10</span>) 
<span style="color:#408080;font-style:italic"># or the following</span>
<span style="color:#408080;font-style:italic">#tf.one_hot(np.argmax(predictions.tolist(),axis=1), depth = 10)</span>
y_pred_labels

</code></pre></div><h2 id="confusion-matrix">Confusion matrix</h2>
<p>We construct a 10x10 confusion matrix for our labels.  The precision
of each word can be seen by dividing the values in the dark squares which are our
true positives with the total of each column.  The precision says how our classifier
can identify words just as a human would by hearing.  Better precision translates
to better identification capability.
The word &lsquo;on&rsquo; has the highest precision value of <code>254/(254+5+1+2+2+1+4+1)=0.94</code> and the
word &lsquo;left&rsquo; has the lowest precision value of <code>203/(203+1+3+1+3+1+11+5+4+3)=0.86</code>.</p>
<p><img src="confusion_matrix.png" alt="Confusion matrix"></p>
<h2 id="conclusion">Conclusion</h2>
<p>Sound classification has a close resemblance to image classification.  The
precision of our classifier bears a relation to how humans listen and identify
spoken words &ndash; it seems at least as good!</p>
<p>Thanks to spectrograms, a CNN can classify sound.  It is far easier on our
part to deal with them rather than with waveforms.  You can see the implementation
in my <a href="https://www.kaggle.com/code/atrijtalgery/spch-cmds-cnn-3x-augm-n-custom-loader-tpu-ready">Kaggle notebook</a>.</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Hashing functions ensure uniform distribution of keys across the hash value
space.  The size of the hash is too big and is reduced to a smaller
value using the modulo function.  Then, the value is scaled to between 0 to 100 for easy percentage-wise splitting. <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/cnn/" rel="tag">cnn</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/speech/" rel="tag">speech</a>
		</li>
	</ul>
</div>
		</footer>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/review-vs-rating-two-case-studies/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Review vs Rating: Two Case Studies</p>
		</a>
	</div>
</nav>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
<div class="footer__links">
	<a class="footer__link" href="/about/">About</a>
</div>
		<div class="footer__copyright">
			&copy; 2024 Atrij Talgery.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
</body>
</html>