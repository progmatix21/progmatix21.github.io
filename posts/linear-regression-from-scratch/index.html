<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Linear Regression From Scratch - Progmatix 21</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="Progmatix 21" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">Progmatix 21</div>
					<div class="logo__tagline">My learnings: byte by byte</div>
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Linear Regression From Scratch</h1>
			<p class="post__lead">Don&#39;t just tow the line, move it!</p>
			<div class="post__meta meta">
<div class="meta__item-datetime meta__item">
	<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<svg version="1.1" id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" class="meta__icon icon icon-calendar" width="18" height="16" 
	 viewBox="0 0 458 458" style="enable-background:new 0 0 458 458;" xml:space="preserve">
<g>
	<path d="M111.938,135.598c11.046,0,20-8.954,20-20V21.597c0-11.046-8.954-20-20-20s-20,8.954-20,20v94.001
		C91.938,126.644,100.892,135.598,111.938,135.598z"/>
	<path d="M346.063,135.598c11.046,0,20-8.954,20-20V21.597c0-11.046-8.954-20-20-20s-20,8.954-20,20v94.001
		C326.063,126.644,335.017,135.598,346.063,135.598z"/>
	<path d="M443,82.403h-46.938c0,12.42,0,20.918,0,33.195c0,27.614-22.386,50-50,50c-27.614,0-50-22.386-50-50
		c0-12.281,0-20.771,0-33.195H161.938c0,12.42,0,20.918,0,33.195c0,27.614-22.386,50-50,50c-27.614,0-50-22.386-50-50
		c0-12.281,0-20.771,0-33.195H15c-8.284,0-15,6.716-15,15v89.641h458V97.403C458,89.119,451.284,82.403,443,82.403z"/>
	<path d="M0,441.403c0,8.284,6.716,15,15,15h428c8.284,0,15-6.716,15-15V227.044H0V441.403z M244.191,313.046
		c0-6.036,3.59-11.504,9.127-13.907c7.49-3.249,16.316-9.028,21.176-13.393c2.782-2.499,6.38-3.88,10.121-3.88h19.254
		c4.395,0,7.957,3.563,7.957,7.958v80.903h11.445c8.275,0,14.984,6.708,14.984,14.984c0,8.275-6.708,14.984-14.984,14.984h-59.934
		c-8.275,0-14.984-6.708-14.984-14.984c0-8.275,6.708-14.984,14.984-14.984h14.567V317.66c-2.163,2.886-6.797,6.077-12.243,8.754
		c-4.618,2.27-10.076,2.008-14.445-0.711c-4.369-2.718-7.025-7.501-7.025-12.647V313.046z M123.295,390.677
		c2.691-29.175,18.127-37.168,47.125-52.83c4.622-2.493,12.811-7.06,15.92-11.134c3.842-5.044,1.97-18.001-14.359-18.001
		c-5.549,0-11.094,1.106-17.704,5.967c-6.262,4.605-14.984,3.637-20.067-2.244l-0.594-0.688c-3.026-3.501-4.246-8.216-3.31-12.747
		s3.925-8.371,8.09-10.387c22.329-10.811,56.654-13.101,73.853,0.952c7.977,6.523,11.966,15.332,11.966,26.43
		c0,32.434-40.129,38.997-54.315,54.731h41.622c8.275,0,14.984,6.708,14.984,14.984c0,8.275-6.708,14.984-14.984,14.984h-79.1
		c-2.577,0-5.036-1.095-6.772-3C123.914,395.789,123.059,393.243,123.295,390.677z"/>
</g>
</svg>
<time class="meta__text" datetime="2023-09-27T20:02:52&#43;05:30">September 27, 2023</time></div>
<div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2l1 2h8v11h-16v-13z"/></svg><span class="meta__text"><a class="meta__link" href="/categories/data-science/" rel="category">Data Science</a>
	</span>
</div><div class="meta__item-categories meta__item">
	<span class="meta__text">
		<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>1597 words/8 min read
	</span>
</div>
</div>
		</header>
		
		
		
<div class="post__toc toc">
	<div class="toc__title">Page content</div>
	<div class="toc__menu">
	    <details>
	        <summary>Click to toggle</summary>
		        <nav id="TableOfContents">
  <ul>
    <li><a href="#what-is-linear-regression">What is linear regression?</a></li>
    <li><a href="#how-does-a-linear-regression-work">How does a linear regression work?</a>
      <ul>
        <li><a href="#the-process-of-learning-weights">The process of learning weights</a></li>
        <li><a href="#the-implementation">The Implementation</a></li>
      </ul>
    </li>
    <li><a href="#making-predictions">Making predictions</a></li>
  </ul>
</nav>
		    </details>
	</div>
</div>
<div class="content post__content clearfix">
			<style>
#myBtn {
  display: none;
  position: fixed;
  bottom: 20px;
  right: 30px;
  z-index: 99;
  font-size: 36px;
  font-weight: bold;
  border: none;
  outline: none;
  background-color: #444;
  color: white;
  cursor: pointer;
  padding: 15px;
  border-radius: 50%;
}

#myBtn:hover {
  background-color: #008080;
}
</style>

<button onclick="topFunction()" id="myBtn" title="Go to top">&#9650;</button>


<script>

let mybutton = document.getElementById("myBtn");


window.onscroll = function() {scrollFunction()};

function scrollFunction() {
  if (document.body.scrollTop > 1000 || document.documentElement.scrollTop > 1000) {
    mybutton.style.display = "block";
  } else {
    mybutton.style.display = "none";
  }
}


function topFunction() {
  document.body.scrollTop = 0;
  document.documentElement.scrollTop = 0;
}
</script>



<!--https://latexeditor.lagrida.com/, https://www.mathcha.io/editor -->
<p>Imagine you are buying a car and you want to know about its mileage.  You don&rsquo;t
want to go for the user reviews or the company&rsquo;s claim of mileage.  The option
you are left with is to predict the mileage all by yourself.  So, if you are
an interested data scientist, why not give it a try?</p>
<p>This is where linear regression comes in to help you.  All you have to do is
collect the necessary data and fit your model to it!</p>
<p>Are you looking to buy a house and overwhelmed to see the house prices?  You
obviously don&rsquo;t want to ask the seller about the rise in prices as he might
misguide you.  If the curiosity to know kills you, then you might search for
house prices over a period of years and observe the trend.  An even better
way would be to take the year,area and other attributes as predictors
and price as the response variable, and fit a regression model.</p>
<h2 id="what-is-linear-regression">What is linear regression?</h2>
<p>From the above examples, we have attributes of the car such as horsepower,
weight and a &lsquo;variable of interest&rsquo; such as mileage as the
response variable.  Similarly, we have attributes such as house area, house
facilities and house price as the &lsquo;variable of interest&rsquo;.</p>
<p>The attributes are called <em>predictors</em> and the &lsquo;variable of interest&rsquo; is called
the <em>response</em> variable.</p>
<p>Linear regression has two parts:</p>
<ul>
<li>building/training the model with the available data (both predictors and response variable), and</li>
<li>using the model to predict the response variable given arbitrary predictors &ndash;
usually those the model hasn&rsquo;t seen.</li>
</ul>
<p>Linear regression is not limited to modelling a linear relationship between the
predictors and the response variable.  Basis function regression allows us to
model non-linear relationship between predictors and a response variable.
Polynomial regression is a kind of basis function regression.  However, this
regression is still &lsquo;linear&rsquo; because it is a weighted linear combination of
features, some of them arrived via basis functions.</p>
<p>Here, we just look at modelling a linear relationship between the predictors
and response variables using OLS (Ordinary Least Squares) and Gradient Descent.</p>
<h2 id="how-does-a-linear-regression-work">How does a linear regression work?</h2>
<p>A linear regression model can be represented thus:</p>
<p>$$
\hat{y} =\mathbf{xw}   \tag{1}
$$</p>
<p>where \(\hat{y}\) is the predicted response variable, \(\mathbf{x}\) is the
predictors matrix (observations in rows and features in columns) and \(\mathbf{w}\)
is the <em>column</em> vector of weights learned while building the model.</p>
<h3 id="the-process-of-learning-weights">The process of learning weights</h3>
<p>The process can be summarized as follows:</p>
<ul>
<li>define an error function</li>
<li>compute a candidate prediction using random starting weights</li>
<li>iteratively reduce the error by recomputing the weights using gradient descent</li>
</ul>
<p>Our error/loss function is</p>
<p>$$
E(\mathbf{w}) = \frac{1}{2m} \sum_{i=1}^{m} (\mathbf{x}_{i}\mathbf{w}-y_{i})^2  \tag{2}
$$</p>
<p>where, \(\mathbf{x}_{i}\) is the row vector for the \(i^{th}\) observation and
\(\mathbf{w}\) is the weights column vector.  \(y_i\) is the  response
variable for the \(i^{th}\) observation.  We choose a quadratic function
because it has a unique global minimum which can be arrived at by gradient descent.</p>
<p>The gradient of the loss function with respect to the weights is
used to compute the small increments/updates to the weight vector.</p>
<p>$$
\Delta{\mathbf{w}}  =  -\alpha\nabla{E(\mathbf{w})} \tag{3}
$$</p>
<p>The constant \(\alpha\) is called the learning rate and it is chosen small
enough so that the gradient descent doesn&rsquo;t cross over the minima.  Since the
gradient is defined as the direction of steepest ascent, the negative
sign signifies the steepest gradient descent.</p>
<p>Deriving the gradient descent expression, we can write the update to individual
weights as follows:</p>
<p>$$
\Delta{w_j} = -\alpha \sum_{i=1}^{m} (\mathbf{x}_{i}\mathbf{w}-y_{i})x_{ij}  \tag{4}
$$</p>
<p>and</p>
<p>$$
w_j = w_j + \Delta{w_j}   \tag{5}
$$</p>
<p>Here, \(w_j\) is the \(j^{th}\) weight and \(x_{ij}\) is the
\(j^{th}\)feature in the \(i^{th}\) row/observation.  To compute the update
to the \(j^{th}\) weight, we sum up error attributable to this weight across
all observations.  This is called standard gradient descent.</p>
<p>For ease of implementation equation (4) can also be written in a vectorized form
as:</p>
<p>$$
\Delta{w_j} = -\alpha \cdot sum((\mathbf{xw} - y) \circ \mathbf{x}_{*j})  \tag{6}
$$</p>
<p>This is the sum of the hadamard product vector of residue for each row and the
\(j^{th}\) column of the feature matrix (or the vector for the \(j^{th}\)
feature).</p>
<p>However, standard gradient descent is computationally expensive for large datasets.
An alternative method is stochastic gradient descent that updates weights after
each observation.  This method gives results which are arbitrarily close to
standard gradient descent.</p>
<h3 id="the-implementation">The Implementation</h3>
<p>We code a function for gradient descent and use it to plot a straight line
regression and later a non-linear relationship using basis function regression.</p>
<div class="highlight"><div style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="display:block;width:100%;background-color:#e5e5e5"><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span></span><span style="display:block;width:100%;background-color:#e5e5e5"><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span></span><span style="display:block;width:100%;background-color:#e5e5e5"><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span></span><span style="display:block;width:100%;background-color:#e5e5e5"><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span></span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">41
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">42
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">43
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">44
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">45
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">46
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">47
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">48
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">49
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">50
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">51
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">52
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">53
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">54
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">55
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">56
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">57
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">58
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">59
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">60
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">61
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">62
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">63
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">64
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">65
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">66
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">67
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">68
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">69
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">70
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">71
</span><span style="display:block;width:100%;background-color:#e5e5e5"><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">72
</span></span><span style="display:block;width:100%;background-color:#e5e5e5"><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">73
</span></span><span style="display:block;width:100%;background-color:#e5e5e5"><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">74
</span></span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">75
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">76
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">77
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">78
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">79
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">80
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">81
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">82
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">83
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">84
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">85
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">86
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">87
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#408080;font-style:italic">#!/usr/bin/env python3</span>
<span style="color:#408080;font-style:italic"># -*- coding: utf-8 -*-</span>
<span style="color:#ba2121">&#34;&#34;&#34;
</span><span style="color:#ba2121">Created on Sun Oct  1 11:30:32 2023
</span><span style="color:#ba2121">
</span><span style="color:#ba2121">@author: github/progmatix21
</span><span style="color:#ba2121">&#34;&#34;&#34;</span>

<span style="color:#008000;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">sklearn.datasets</span> <span style="color:#008000;font-weight:bold">import</span> make_regression
<span style="color:#008000;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">sklearn.preprocessing</span> <span style="color:#008000;font-weight:bold">import</span> PolynomialFeatures
<span style="color:#008000;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">matplotlib.pyplot</span> <span style="color:#008000;font-weight:bold">as</span> <span style="color:#00f;font-weight:bold">plt</span>
<span style="color:#008000;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">numpy</span> <span style="color:#008000;font-weight:bold">as</span> <span style="color:#00f;font-weight:bold">np</span>

<span style="color:#008000;font-weight:bold">def</span> <span style="color:#00f">ols_gd</span>(X,y, alpha<span style="color:#666">=</span><span style="color:#666">0.001</span>, n_iter<span style="color:#666">=</span><span style="color:#666">50</span>):
    <span style="color:#ba2121">&#39;&#39;&#39;
</span><span style="color:#ba2121">    X: feature matrix(mxn), m rows of n features.
</span><span style="color:#ba2121">    y: response variable array of length m
</span><span style="color:#ba2121">    alpha: the learning rate
</span><span style="color:#ba2121">    n_iter: number of iterations of gradient descent
</span><span style="color:#ba2121">    returns weight vector
</span><span style="color:#ba2121">    &#39;&#39;&#39;</span>
    <span style="color:#408080;font-style:italic"># Insert a column of 1s into X to signify x0 corresponding to w0</span>
    _X <span style="color:#666">=</span> np<span style="color:#666">.</span>insert(X,<span style="color:#666">0</span>,<span style="color:#666">1</span>,axis<span style="color:#666">=</span><span style="color:#666">1</span>)
    <span style="color:#408080;font-style:italic"># number of weights is same as number of features</span>
    n_w <span style="color:#666">=</span> _X<span style="color:#666">.</span>shape[<span style="color:#666">1</span>]
    <span style="color:#408080;font-style:italic"># Create a column vector that includes w0 and intialize to 1s</span>
    w <span style="color:#666">=</span> np<span style="color:#666">.</span>array([<span style="color:#666">0.1</span>]<span style="color:#666">*</span>n_w, dtype<span style="color:#666">=</span><span style="color:#008000">float</span>)<span style="color:#666">.</span>reshape(<span style="color:#666">-</span><span style="color:#666">1</span>,<span style="color:#666">1</span>)
    alpha <span style="color:#666">=</span> alpha  <span style="color:#408080;font-style:italic"># Initialize the learning rate</span>

    history <span style="color:#666">=</span> <span style="color:#008000">list</span>() <span style="color:#408080;font-style:italic"># history to record convergence of weights</span>
    history<span style="color:#666">.</span>append(w<span style="color:#666">.</span>ravel()<span style="color:#666">.</span>copy()) <span style="color:#408080;font-style:italic"># append the initial weights</span>
<span style="display:block;width:100%;background-color:#e5e5e5">    <span style="color:#408080;font-style:italic"># the gradient descent loop</span>
</span><span style="display:block;width:100%;background-color:#e5e5e5">    <span style="color:#008000;font-weight:bold">for</span> _ <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#008000">range</span>(n_iter):
</span><span style="display:block;width:100%;background-color:#e5e5e5">        <span style="color:#008000;font-weight:bold">for</span> j <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#008000">range</span>(w<span style="color:#666">.</span>shape[<span style="color:#666">0</span>]): <span style="color:#408080;font-style:italic"># for each weight: w is a column vector</span>
</span><span style="display:block;width:100%;background-color:#e5e5e5">            w[j,<span style="color:#666">0</span>] <span style="color:#666">+=</span> <span style="color:#666">-</span>alpha<span style="color:#666">*</span>np<span style="color:#666">.</span>sum((np<span style="color:#666">.</span>matmul(_X,w)<span style="color:#666">.</span>ravel()<span style="color:#666">-</span>y)<span style="color:#666">*</span>_X[:,j])
</span>        history<span style="color:#666">.</span>append(w<span style="color:#666">.</span>ravel()<span style="color:#666">.</span>copy())

    <span style="color:#008000;font-weight:bold">return</span> w<span style="color:#666">.</span>ravel(),history

<span style="color:#008000;font-weight:bold">if</span> __name__ <span style="color:#666">==</span> <span style="color:#ba2121">&#39;__main__&#39;</span>:
    <span style="color:#408080;font-style:italic"># Generate the data</span>
    w0 <span style="color:#666">=</span> <span style="color:#666">50</span>  <span style="color:#408080;font-style:italic"># bias is w0</span>
    X,y,c <span style="color:#666">=</span> make_regression(n_samples<span style="color:#666">=</span><span style="color:#666">100</span>,n_features<span style="color:#666">=</span><span style="color:#666">1</span>,noise<span style="color:#666">=</span><span style="color:#666">4</span>,bias<span style="color:#666">=</span>w0,coef<span style="color:#666">=</span>True)

    <span style="color:#408080;font-style:italic"># Run the regression</span>
    weights, history <span style="color:#666">=</span> ols_gd(X,y,n_iter<span style="color:#666">=</span><span style="color:#666">50</span>)

    <span style="color:#408080;font-style:italic"># Plot the convergence of weights</span>
    plt<span style="color:#666">.</span>plot(history, <span style="color:#ba2121">&#39;-&#39;</span>)
    plt<span style="color:#666">.</span>legend([f<span style="color:#ba2121">&#39;$w_{i}$&#39;</span> <span style="color:#008000;font-weight:bold">for</span> i <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#008000">range</span>(<span style="color:#008000">len</span>(weights))])
    plt<span style="color:#666">.</span>title(<span style="color:#ba2121">&#39;Converging weights&#39;</span>)
    plt<span style="color:#666">.</span>grid()
    plt<span style="color:#666">.</span>show()
    <span style="color:#408080;font-style:italic"># Plot data and trace of the line</span>
    <span style="color:#008000;font-weight:bold">if</span> X<span style="color:#666">.</span>shape[<span style="color:#666">1</span>] <span style="color:#666">==</span> <span style="color:#666">1</span>:
        plt<span style="color:#666">.</span>scatter(X[:,<span style="color:#666">0</span>],y,c<span style="color:#666">=</span>y,cmap<span style="color:#666">=</span><span style="color:#ba2121">&#39;viridis&#39;</span>)
        <span style="color:#008000;font-weight:bold">for</span> ws <span style="color:#a2f;font-weight:bold">in</span> history:
            plt<span style="color:#666">.</span>plot(X[:,<span style="color:#666">0</span>], [ws[<span style="color:#666">0</span>]<span style="color:#666">+</span>np<span style="color:#666">.</span>dot(Xrow,ws[<span style="color:#666">1</span>:]) <span style="color:#008000;font-weight:bold">for</span> Xrow <span style="color:#a2f;font-weight:bold">in</span> X], color<span style="color:#666">=</span><span style="color:#ba2121">&#39;Blue&#39;</span>,alpha<span style="color:#666">=</span><span style="color:#666">0.1</span>)
        plt<span style="color:#666">.</span>legend([<span style="color:#ba2121">&#39;data&#39;</span>,<span style="color:#ba2121">&#39;prediction&#39;</span>])
        plt<span style="color:#666">.</span>title(<span style="color:#ba2121">&#39;OLS straight line regression&#39;</span>)
        plt<span style="color:#666">.</span>show()

    <span style="color:#408080;font-style:italic"># Basis function regression to model a non-linear function</span>
    <span style="color:#408080;font-style:italic"># Generate the function</span>
    n_points <span style="color:#666">=</span> <span style="color:#666">100</span>
    rng <span style="color:#666">=</span> np<span style="color:#666">.</span>random<span style="color:#666">.</span>RandomState()
    x <span style="color:#666">=</span> rng<span style="color:#666">.</span>rand(n_points)
    y <span style="color:#666">=</span> np<span style="color:#666">.</span>sin(<span style="color:#666">2</span><span style="color:#666">*</span><span style="color:#666">3.14159</span><span style="color:#666">*</span>x) <span style="color:#666">+</span> <span style="color:#666">0.1</span> <span style="color:#666">*</span> rng<span style="color:#666">.</span>randn(n_points)
    plt<span style="color:#666">.</span>scatter(x, y)

    <span style="color:#408080;font-style:italic"># apply a polynomial basis function to x</span>
<span style="display:block;width:100%;background-color:#e5e5e5">    poly_basis <span style="color:#666">=</span> PolynomialFeatures(<span style="color:#666">9</span>, include_bias<span style="color:#666">=</span>False)
</span><span style="display:block;width:100%;background-color:#e5e5e5">    X <span style="color:#666">=</span> poly_basis<span style="color:#666">.</span>fit_transform(x[:, None])
</span><span style="display:block;width:100%;background-color:#e5e5e5">    
</span>    <span style="color:#408080;font-style:italic"># Run the regression</span>
    weights, history <span style="color:#666">=</span> ols_gd(X,y,n_iter<span style="color:#666">=</span><span style="color:#666">50000</span>,alpha<span style="color:#666">=</span><span style="color:#666">0.005</span>)

    <span style="color:#408080;font-style:italic"># Plot the journey of the curve fitting</span>
    <span style="color:#008000;font-weight:bold">for</span> i, ws <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#008000">enumerate</span>(history, start<span style="color:#666">=</span><span style="color:#666">1</span>):
        <span style="color:#008000;font-weight:bold">if</span> i <span style="color:#a2f;font-weight:bold">in</span> [<span style="color:#666">1</span>,<span style="color:#666">2</span>,<span style="color:#666">3</span>,<span style="color:#666">4</span>,<span style="color:#666">5</span>,<span style="color:#666">10</span>,<span style="color:#666">100</span>,<span style="color:#666">500</span>,<span style="color:#666">1000</span>,<span style="color:#666">2000</span>,<span style="color:#666">3000</span>,<span style="color:#666">4000</span>,<span style="color:#666">5000</span>, \
                 <span style="color:#666">20000</span>,<span style="color:#666">25000</span>,<span style="color:#666">30000</span>,<span style="color:#666">45000</span>,<span style="color:#666">50000</span>]:   <span style="color:#408080;font-style:italic">#plot every nth instance of candidate predictions</span>
            plt<span style="color:#666">.</span>scatter(X[:,<span style="color:#666">0</span>], [ws[<span style="color:#666">0</span>]<span style="color:#666">+</span>np<span style="color:#666">.</span>dot(Xrow,ws[<span style="color:#666">1</span>:]) <span style="color:#008000;font-weight:bold">for</span> Xrow <span style="color:#a2f;font-weight:bold">in</span> X], s<span style="color:#666">=</span><span style="color:#666">10</span>, color<span style="color:#666">=</span><span style="color:#ba2121">&#39;Black&#39;</span>,alpha<span style="color:#666">=.</span><span style="color:#666">1</span>)

    plt<span style="color:#666">.</span>legend([<span style="color:#ba2121">&#39;True&#39;</span>,<span style="color:#ba2121">&#39;Estimated&#39;</span>])
    plt<span style="color:#666">.</span>grid()
    plt<span style="color:#666">.</span>title(<span style="color:#ba2121">&#39;OLS basis function regression&#39;</span>)    
    plt<span style="color:#666">.</span>show()</code></pre></td></tr></table>
</div>
</div>
<p>The first highlighted portion of the code is the core gradient descent implementation
that follows equation (6).</p>
<p><img src="converging_weights.png" width=30% height=30% style="float:left;margin:-20px 0px 0px 0px;" />
<img src="ols_straight_line_reg.png" width=30% height=30% style="float:left;margin: -20px 0px 0px 0px;" /></p>
<p>To illustrate the convergence of weights and how the model evolves to fit the data,
we have taken a regression with one variable.  The figure on the left shows the
convergence of weights and on the right on how the model evolves.  You can note
how the model (the regression line) sidles up to the data (denoted by the scatter
plot) in the darkened portion of the plot.  The evolution of the model corresponds
to the convergence of the weights.</p>
<p>However, &lsquo;linear&rsquo; regression is not limited to modelling linear relationships alone
as we will presently see.</p>
<p><img src="ols_basis_function_reg.png" width=40% height=40% style="float:right;margin:-40px 0px 0px 0px;" /></p>
<p>To show that linear regression can also model non-linear relationships, we
generate data in the form of a noisy sine curve and fit a model to it.</p>
<p>For this, as shown in the second highlighted portion of the code, we generate
polynomial features (our basis function) to feed our OLS model.</p>
<p>The plot on the right shows how the model (black dots) evolves to fit the data
(blue dots) over the successive iterations.</p>
<blockquote>
<p>It is very important to choose the correct combination of the number of iterations
and the learning rate to get a converging model.</p>
</blockquote>
<h2 id="making-predictions">Making predictions</h2>
<p>Given a feature vector or a table of test data, how would you predict your response
variable?   The <code>ols_gd()</code> function returns a vector of weights including the bias
\(w_0\) given the training data <code>X,y</code>. You can train your model and make
predictions as follows:</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model_weights <span style="color:#666">=</span> ols_gd(X_train, y_train)
y_hat <span style="color:#666">=</span> [model_weights[<span style="color:#666">0</span>]<span style="color:#666">+</span>numpy<span style="color:#666">.</span>dot(Xrow,model_weights[<span style="color:#666">1</span>:]) <span style="color:#008000;font-weight:bold">for</span> Xrow <span style="color:#a2f;font-weight:bold">in</span> X_test]
</code></pre></div><p>Remember, if you are applying basis function regression (for modelling a non-
linear relationship), both your training and test data will need to go through
the basis function transformation.  It is assumed that both <code>X_train</code> and <code>X_test</code>
above are basis-function-transormed if necessary.</p>
<p>We started out with trying to predict car mileage and house prices.  If you have
done your data gathering, you are ready to build your regression model.  What&rsquo;s
more, you now understand how it works.</p>
<hr>
<!--
In a given data, we want to plot a graph and plot random points that are
scattered all over the place.  The scattered points do not give any meaningful
information as a whole.  We want a best fitting approximation for those points.
This can be done by fitting a line, either a curved one or a straight one to those
points in such a way that the line cuts through the maximum number of points.
The straight line regression is also known as linear regression and the curved
line regression is also known as logistic regression.


# How does a linear regression work?




Regression consists of the following main types:
1. Linear Regression - Linear regression fits a straight line approximation to
a group of points.  This regression has both dependent and independent variables
as continuous.  A linear regression line can be straight or slightly curved, but
it can never be a sigmoid curve.
2. Logistic Regression - This regression fits a sigmoid function curve to the
group of points.  It has the independent variable as continuous and the independent
variable as categorical.
3. Multiple Regression - This is a form of linear regression having multiple
regression lines.


# Mathematics and different terminology

They are best suited in places where we want to evaluate data based on its
consistency.  The consistency of linear regression is found out by the R score.
For example if the R score is 0.9, it means it is 99% correct leaving only 1%
for errors, whereas an R score of 0.1 means that the model is only 1% correct
and 99% wrong.

Logistic regression, on the other hand is more classification oriented.  This
is because, its dependent variable is always taken as a categorical variable,
usually a binary variable which can be either a 0 or 1.  This is why logistic
regression is a handy method to apply as a simple classification technique.


# Linear Regression in action

The key differences between the types of regression is that Linear regression
concentrates more on evaluating the model as is and returns a regression score.
A good regression score denotes how good the model is at prediction.

On the other hand, Logistic regression evaluates data by classification.

# Conclusion



# How are they used?

# Methods of travelling along the line or curve

There are mainly two ways to travel along the line or curve namely, gradient
ascent and gradient descent.  They work on the concept of derivatives.  Gradient
ascent is used for curves that have a sharp ascent and gradient descent is used
for the descending ones.  

-->
		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/regression/" rel="tag">regression</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/ols/" rel="tag">OLS</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/howto/" rel="tag">howto</a>
		</li>
	</ul>
</div>
		</footer>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/mergesort-cutaway/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Mergesort Cutaway</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/roc-curve-step-by-step/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Roc Curve Step by Step</p>
		</a>
	</div>
</nav>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
<div class="footer__links">
	<a class="footer__link" href="/about/">About</a>
</div>
		<div class="footer__copyright">
			&copy; 2025 Atrij Talgery.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
</body>
</html>